{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList ACL 2020\n",
    "\n",
    "# One sentence summary:\n",
    "# Method to test the behaviour of NLP models through different experiments on small\n",
    "# manufactured example test sets.\n",
    "\n",
    "# Criticism:\n",
    "# The authors focus mainly on English NLP models and show that the CheckList analyses\n",
    "# uncover errors in commercial and openly available models. However, there is a wide usage\n",
    "# of multilingual models in the NLP community for multilingual or low-resource experiments\n",
    "# which makes an error analysis of multilingual models relevant and interesting.\n",
    "\n",
    "# Planned Implementation:\n",
    "# The plan is to take a multilingual model (e.g., mBERT) and create a Minimum Functionality\n",
    "# Test (MFT) for testing how the model handles negations in parallel Sentiment Analysis\n",
    "# sentences in English (like \"It was bad.\", \"It was good.\", \"It was not good.\"), German (\"Es war\n",
    "# schlecht.\", \"Es war gut.\", \"Es war nicht gut.\") and Bavarian (\"Des war schlecht.\", \"Des war\n",
    "# guad.\", \"Des war ned guad.\"). The experiments should then show whether the CheckList\n",
    "# methods can also be used in a multilingual setup and whether mBERT's behaviour changes\n",
    "# for different languages.\n",
    "\n",
    "# Feedback:\n",
    "# The proposal extends CheckList to other languages and implements a Minimum Functionality Test comparing English, German and Bavarian.\n",
    "# Looks good. Approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# research questions:\n",
    "# can CheckList be applied to multilingual models? (only tested on English models)\n",
    "# can CheckList be used to compare model capabilities across languages? (here: has the model different behaviour when handling negations in different languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources\n",
    "\n",
    "# code for data creation partly taken from \n",
    "# https://github.com/marcotcr/checklist/blob/master/notebooks/tutorials/1.%20Generating%20data.ipynb\n",
    "# with major changes\n",
    "\n",
    "# code for test creation and execution partly taken from\n",
    "# https://github.com/marcotcr/checklist/blob/master/notebooks/tutorials/3.%20Test%20types%2C%20expectation%20functions%2C%20running%20tests.ipynb and\n",
    "# https://github.com/marcotcr/checklist/blob/master/notebooks/tutorials/4.%20The%20CheckList%20process.ipynb\n",
    "# with minor changes\n",
    "\n",
    "# code for model training taken partly from \n",
    "# https://huggingface.co/blog/sentiment-analysis-python\n",
    "# with minor changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes\n",
    "# hard to find adjective pairs where standard German and Bavarian are different -> focus on only three pairs per sentiment\n",
    "\n",
    "# test sentences only work with predicative adjektive constructions to avoid issues with inflection of the pre-chosen adjectives in standard German and Bavarian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import checklist\n",
    "from checklist.editor import Editor\n",
    "from checklist.test_types import MFT\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise Editor objects\n",
    "eng_editor = Editor() # default language is English\n",
    "deu_editor = Editor(language=\"german\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English adj suggestions: \n",
      "['great', 'good', 'wonderful', 'beautiful', 'fantastic', 'terrible', 'fascinating', 'big', 'nice', 'bad', 'long', 'fun', 'scary', 'terrific', 'tough', 'short', 'serious', 'huge', 'remarkable', 'new'] \n",
      "\n",
      "German adj suggestions: \n",
      "['gutes', 'neues', 'großes', 'kleines', 'starkes', 'politisches', 'anderes', 'wichtiges', 'ganzes', 'besonderes', 'altes', 'deutsches', 'eigenes', 'super', 'weiteres', 'solches', 'zweites', 'einziges', 'letztes', 'erstes']\n",
      "['guter', 'schlechter', 'großer', 'schöner', 'super', 'wichtiger', 'deutscher', 'schwieriger', 'historischer', 'neuer', 'kleiner', 'politischer', 'typischer', 'anderer', 'besonderer', 'amerikanischer', 'klassischer', 'einfacher', 'internationaler', 'langer']\n"
     ]
    }
   ],
   "source": [
    "# get adj suggestions from the CheckList Editors in German and English\n",
    "print(\"English adj suggestions: \")\n",
    "print(eng_editor.suggest(\"This is a {mask} {noun}.\", noun=[\"book\", \"movie\"])[:20], \"\\n\") \n",
    "\n",
    "print(\"German adj suggestions: \")\n",
    "print(deu_editor.suggest(\"Das ist ein {mask} Buch.\")[:20]) \n",
    "print(deu_editor.suggest(\"Das ist ein {mask} Film.\")[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel adjective lists for data creation\n",
    "eng_pos = [\"good\", \"nice\", \"great\"]\n",
    "deu_pos = [\"gut\", \"schön\", \"super\"]\n",
    "bar_pos = [\"guad\", \"schee\", \"subba\"]\n",
    "\n",
    "eng_neg = [\"bad\", \"boring\", \"stupid\"]\n",
    "deu_neg = [\"schlecht\", \"langweilig\", \"blöd\"]\n",
    "bar_neg = [\"schlecht\", \"fad\", \"bled\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English noun suggestions: \n",
      "['game', 'one', 'stuff', 'book', 'article', 'video', 'post', 'movie', 'code', 'thing']\n",
      "['post', 'article', 'game', 'stuff', 'video', 'movie', 'list', 'shit', 'thing', 'book'] \n",
      "\n",
      "German noun suggestions: \n",
      "Der ['Preis', 'Job', 'Platz', 'Service', 'Abend', 'Wind', 'Rest', 'Sieg', 'Auftritt', 'Eindruck']\n",
      "Der ['Rest', 'Job', 'Film', 'Ton', 'Fußball', 'Mann', 'Preis', 'Markt', 'Anfang', 'Fall']\n",
      "Die ['Stimmung', 'Atmosphäre', 'Situation', 'Sache', 'Resonanz', 'Lage', 'Idee', 'Antwort', 'Qualität', 'Zusammenarbeit']\n",
      "Die ['Stimmung', 'Sache', 'Situation', 'Lage', 'Welt', 'Idee', 'Musik', 'Antwort', 'Geschichte', 'Zeit']\n",
      "Das ['Ergebnis', 'Wetter', 'alles', 'Leben', 'Angebot', 'hier', 'Spiel', 'Resultat', 'Stadion', 'Essen']\n",
      "Das ['Ergebnis', 'alles', 'Wetter', 'Leben', 'aber', 'Spiel', 'Ganze', 'hier', 'Auto', 'Geschäft']\n"
     ]
    }
   ],
   "source": [
    "# get noun suggestions from the CheckList Editors in German and English\n",
    "# english suggestions\n",
    "print(\"English noun suggestions: \")\n",
    "print(eng_editor.suggest(\"This {mask} is {pos}.\", pos=eng_pos)[:10]) \n",
    "print(eng_editor.suggest(\"This {mask} is {neg}.\", neg=eng_neg)[:10], \"\\n\") \n",
    "\n",
    "# suggestions for standard German with different determiners\n",
    "print(\"German noun suggestions: \")\n",
    "print(\"Der\", deu_editor.suggest(\"Der {mask} ist {pos}.\", pos=deu_pos)[:10])\n",
    "print(\"Der\", deu_editor.suggest(\"Der {mask} ist {neg}.\", neg=deu_neg)[:10])\n",
    "print(\"Die\", deu_editor.suggest(\"Die {mask} ist {pos}.\", pos=deu_pos)[:10])\n",
    "print(\"Die\", deu_editor.suggest(\"Die {mask} ist {neg}.\", neg=deu_neg)[:10])\n",
    "print(\"Das\", deu_editor.suggest(\"Das {mask} ist {pos}.\", pos=deu_pos)[:10])\n",
    "print(\"Das\", deu_editor.suggest(\"Das {mask} ist {neg}.\", neg=deu_neg)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel noun lists for data creation\n",
    "# picked from Editor suggestions in such a way that mostly all words are different\n",
    "eng_noun = [\"game\", \"site\", \"picture\", \"book\", \"story\", \"man\", \"world\", \"city\", \"time\", \"weather\", \"life\"]\n",
    "\n",
    "# standard German and Bavarian examples with determiners to avoid errors\n",
    "deu_noun = [\n",
    "    (\"Das\", \"Spiel\"), (\"Die\", \"Seite\"), (\"Das\", \"Bild\"), (\"Das\", \"Buch\"), (\"Die\", \"Geschichte\"), \n",
    "    (\"Der\", \"Mann\"), (\"Die\", \"Welt\"), (\"Die\", \"Stadt\"), (\"Die\", \"Zeit\"), (\"Das\", \"Wetter\"), (\"Das\", \"Leben\")\n",
    "    ]\n",
    "\n",
    "# bavarian determiners are with spaces to handle \"d'\" and \"s'\" determiners\n",
    "bar_noun = [\n",
    "    (\"Des \", \"Spui\"), (\"De \", \"Seitn\"), (\"Des \", \"Buidl\"), (\"Des \", \"Buach\"), (\"De \", \"Gschicht\"), \n",
    "    (\"Der \", \"Mo\"), (\"D'\", \"Weid\"), (\"D'\", \"Stod\"), (\"D'\", \"Zeid\"), (\"S'\", \"Weda\"), (\"S'\", \"Lebm\")\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create english data\n",
    "eng = eng_editor.template(\"The {noun} is not {adj}.\", noun=eng_noun, adj=eng_pos, labels=0) # not pos = negative\n",
    "eng += eng_editor.template(\"The {noun} is not {adj}.\", noun=eng_noun, adj=eng_neg, labels=1) # not neg = positive\n",
    "\n",
    "# create german data\n",
    "deu = deu_editor.template(\"{detnoun[0]} {detnoun[1]} ist nicht {adj}.\", detnoun=deu_noun, adj=deu_pos, labels=0)\n",
    "deu += deu_editor.template(\"{detnoun[0]} {detnoun[1]} ist nicht {adj}.\", detnoun=deu_noun, adj=deu_neg, labels=1)\n",
    "\n",
    "# create bavarian data\n",
    "bar = deu_editor.template(\"{detnoun[0]}{detnoun[1]} is ned {adj}.\", detnoun=bar_noun, adj=bar_pos, labels=0)\n",
    "bar += deu_editor.template(\"{detnoun[0]}{detnoun[1]} is ned {adj}.\", detnoun=bar_noun, adj=bar_neg, labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise minimum functionality tests\n",
    "eng_test = MFT(**eng, name=\"English Negations\", capability=\"Negation\", description=\"Litotes sentences to test negation capabilities.\")\n",
    "deu_test = MFT(**deu, name=\"Standard German Negations\", capability=\"Negation\", description=\"Litotes sentences to test negation capabilities.\")\n",
    "bar_test = MFT(**bar, name=\"Bavarian Negations\", capability=\"Negation\", description=\"Litotes sentences to test negation capabilities.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc73d811bbe4349b36b53158963bfbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6f947484484ec883662d9eebba26b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf95a25ade34d948cf06b6977aa32be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7816a9c99f648a282e7bd5382e0cf07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac35544e2f1496a8e2c8f47ceda8e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c937a391b7f64b8b87abc641f113d126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/711M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n",
      "/home/miriam/Music/trustenv/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load finetuned mBERT for sentiment analysis\n",
    "# model_name = \"./models/mBERT_sentiment\"\n",
    "model_name = \"mi-winkler/SentiMBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# initialise pipeline for predictions\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=device, return_all_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on test data\n",
    "lbl2idx = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "\n",
    "def predict(test):\n",
    "    # read data and predict\n",
    "    data = test.to_raw_examples() # necessary for internal CheckList structures (self.result_indexes)\n",
    "    raw_preds = pipe(data)\n",
    "\n",
    "    preds = []\n",
    "    confs = []\n",
    "\n",
    "    # write results in correct CheckList format to a file\n",
    "    for result in raw_preds:\n",
    "        negative = result[0]\n",
    "        neutral = result[1]\n",
    "        positive = result[2]\n",
    "\n",
    "        max_pred = max([negative, neutral, positive], key=lambda x: x[\"score\"])\n",
    "        max_label = max_pred[\"label\"]\n",
    "\n",
    "        # prediction, negative_score, neutral_score, positive_score\n",
    "        preds.append(lbl2idx[max_label])\n",
    "        confs.append(np.array([negative[\"score\"], neutral[\"score\"], positive[\"score\"]]))\n",
    "\n",
    "    return preds, confs\n",
    "\n",
    "eng_preds, eng_confs = predict(eng_test)\n",
    "deu_preds, deu_confs = predict(deu_test)\n",
    "bar_preds, bar_confs = predict(bar_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      66\n",
      "Fails (rate):    50 (75.8%)\n",
      "\n",
      "Example fails:\n",
      "0.6 0.3 0.1 The game is not boring.\n",
      "----\n",
      "0.6 0.3 0.1 The weather is not boring.\n",
      "----\n",
      "0.9 0.1 0.0 The man is not stupid.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "eng_test.run_from_preds_confs(eng_preds, eng_confs, overwrite=True)\n",
    "\n",
    "eng_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      66\n",
      "Fails (rate):    33 (50.0%)\n",
      "\n",
      "Example fails:\n",
      "0.8 0.1 0.0 Das Spiel ist nicht blöd.\n",
      "----\n",
      "0.6 0.3 0.1 Die Geschichte ist nicht langweilig.\n",
      "----\n",
      "0.7 0.3 0.1 Die Seite ist nicht langweilig.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "deu_test.run_from_preds_confs(deu_preds, deu_confs, overwrite=True)\n",
    "\n",
    "deu_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases:      66\n",
      "Fails (rate):    33 (50.0%)\n",
      "\n",
      "Example fails:\n",
      "0.9 0.1 0.0 D'Weid is ned schlecht.\n",
      "----\n",
      "0.8 0.1 0.1 S'Lebm is ned schlecht.\n",
      "----\n",
      "0.8 0.1 0.1 S'Weda is ned bled.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "bar_test.run_from_preds_confs(bar_preds, bar_confs, overwrite=True)\n",
    "\n",
    "bar_test.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0, 2, 2, 0, 2]\n",
      "Confidences: [array([0.44630149, 0.25521377, 0.29848468]), array([0.17414553, 0.14964208, 0.67621237]), array([0.28616518, 0.242695  , 0.47113985]), array([0.46459395, 0.29296777, 0.24243827]), array([0.23632866, 0.1914082 , 0.57226312])]\n",
      "len preds:  66\n",
      "len confs:  66\n",
      "Test cases:      66\n",
      "Fails (rate):    54 (81.8%)\n",
      "\n",
      "Example fails:\n",
      "0.9 0.1 0.0 The game is not stupid.\n",
      "----\n",
      "0.2 0.2 0.6 The picture is not nice.\n",
      "----\n",
      "0.2 0.2 0.6 The city is not nice.\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# from checklist.test_types import MFT\n",
    "# import numpy as np\n",
    "\n",
    "# eng_test = MFT(**eng, name=\"English Negations\", capability=\"Negation\", description=\"Litotes sentences to test negation capabilities.\")\n",
    "\n",
    "# eng_data = eng_test.to_raw_examples()\n",
    "\n",
    "# # with open(raw_file_path, \"r\") as f:\n",
    "# #     eng_data = f.read().splitlines()\n",
    "\n",
    "# raw_preds = pipe(eng_data)\n",
    "# preds = []\n",
    "# confs = []\n",
    "\n",
    "# for result in raw_preds:\n",
    "#     negative = result[0]\n",
    "#     neutral = result[1]\n",
    "#     positive = result[2]\n",
    "\n",
    "#     max_pred = max([negative, neutral, positive], key=lambda x: x[\"score\"])\n",
    "#     max_label = max_pred[\"label\"]\n",
    "\n",
    "#     # prediction, negative_score, neutral_score, positive_score\n",
    "#     preds.append(lbl2idx[max_label])\n",
    "#     confs.append(np.array([negative[\"score\"], neutral[\"score\"], positive[\"score\"]]))\n",
    "\n",
    "\n",
    "# from checklist.abstract_test import read_pred_file\n",
    "# from checklist.test_types import MFT\n",
    "\n",
    "# # Assuming eng_test is already created as an MFT instance\n",
    "\n",
    "# # Read predictions and confidences from the file\n",
    "# # preds, confs = read_pred_file(\"/tmp/preds_eng.txt\", file_format=\"softmax\")\n",
    "\n",
    "# # Verify the parsed predictions and confidences\n",
    "# print(\"Predictions:\", preds[:5])\n",
    "# print(\"Confidences:\", confs[:5])\n",
    "\n",
    "# print(\"len preds: \", len(preds))\n",
    "# print(\"len confs: \", len(confs))\n",
    "\n",
    "# if preds is None or confs is None:\n",
    "#     print(\"Error: The predictions or confidences are None. Please check the file format.\")\n",
    "\n",
    "# # Run the test using the loaded predictions and confidences\n",
    "# eng_test.run_from_preds_confs(preds, confs, overwrite=True)\n",
    "\n",
    "# # Print the summary of the results\n",
    "# eng_test.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trustenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
