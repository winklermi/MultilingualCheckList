{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper: Beyond Accuracy: Behavioral Testing of NLP Models with CheckList ACL 2020\n",
    "\n",
    "# One sentence summary:\n",
    "# Method to test the behaviour of NLP models through different experiments on small\n",
    "# manufactured example test sets.\n",
    "\n",
    "# Criticism:\n",
    "# The authors focus mainly on English NLP models and show that the CheckList analyses\n",
    "# uncover errors in commercial and openly available models. However, there is a wide usage\n",
    "# of multilingual models in the NLP community for multilingual or low-resource experiments\n",
    "# which makes an error analysis of multilingual models relevant and interesting.\n",
    "\n",
    "# Planned Implementation:\n",
    "# The plan is to take a multilingual model (e.g., mBERT) and create a Minimum Functionality\n",
    "# Test (MFT) for testing how the model handles negations in parallel Sentiment Analysis\n",
    "# sentences in English (like \"It was bad.\", \"It was good.\", \"It was not good.\"), German (\"Es war\n",
    "# schlecht.\", \"Es war gut.\", \"Es war nicht gut.\") and Bavarian (\"Des war schlecht.\", \"Des war\n",
    "# guad.\", \"Des war ned guad.\"). The experiments should then show whether the CheckList\n",
    "# methods can also be used in a multilingual setup and whether mBERT's behaviour changes\n",
    "# for different languages.\n",
    "\n",
    "# Feedback:\n",
    "# The proposal extends CheckList to other languages and implements a Minimum Functionality Test comparing English, German and Bavarian.\n",
    "# Looks good. Approved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# research questions:\n",
    "# can CheckList be applied to multilingual models? (only tested on English models)\n",
    "# can CheckList be used to compare model capabilities across languages? (here: has the model different behaviour when handling negations in different languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources\n",
    "\n",
    "# code for data creation partly taken from \n",
    "# https://github.com/marcotcr/checklist/blob/master/notebooks/tutorials/1.%20Generating%20data.ipynb\n",
    "# with major changes\n",
    "\n",
    "# code for test creation and execution partly taken from\n",
    "# https://github.com/marcotcr/checklist/blob/master/notebooks/tutorials/3.%20Test%20types%2C%20expectation%20functions%2C%20running%20tests.ipynb and\n",
    "# https://github.com/marcotcr/checklist/blob/master/notebooks/tutorials/4.%20The%20CheckList%20process.ipynb\n",
    "# with minor changes\n",
    "\n",
    "# code for model training taken partly from \n",
    "# https://huggingface.co/blog/sentiment-analysis-python\n",
    "# with minor changes\n",
    "\n",
    "# code for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes\n",
    "# hard to find parallel adjective pairs where standard German and Bavarian are different -> focus on only three pairs per sentiment\n",
    "\n",
    "# test sentences only work with predicative adjective constructions to avoid issues with inflection of the pre-chosen adjectives in standard German and Bavarian\n",
    "\n",
    "# the tests give a good first glance at the performance, but no further error informations. \n",
    "# i visualised the predictions per label with matplotlib to get additional information about which labels fail which could be important information.\n",
    "\n",
    "# running the suite with a prediction function does not allow for access to the scores, so if you need the scores themselves, you have to predict to a file\n",
    "# and run the suite from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if pip installation of CheckList fails, execute this cell\n",
    "# %%bash\n",
    "# git clone https://github.com/marcotcr/checklist.git\n",
    "# cd checklist/\n",
    "# pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for modernbert model install this transformers version\n",
    "# !pip install \"git+https://github.com/huggingface/transformers.git@6e0515e99c39444caae39472ee1b2fd76ece32f1\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.editor import Editor\n",
    "from checklist.test_types import MFT\n",
    "from checklist.test_suite import TestSuite\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise CheckList Editor objects\n",
    "eng_editor = Editor() # default language = English\n",
    "deu_editor = Editor(language=\"german\")\n",
    "\n",
    "# initialise CheckList TestSuite for running the tests\n",
    "eng_suite = TestSuite()\n",
    "deu_suite = TestSuite()\n",
    "bar_suite = TestSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linguistic resources\n",
    "# words chosen from the CheckList Editor suggestions\n",
    "\n",
    "# parallel adjective lists for data creation\n",
    "eng_pos = [\"good\", \"nice\", \"great\"]\n",
    "deu_pos = [\"gut\", \"schön\", \"super\"]\n",
    "bar_pos = [\"guad\", \"schee\", \"subba\"]\n",
    "\n",
    "eng_neg = [\"bad\", \"boring\", \"stupid\"]\n",
    "deu_neg = [\"schlecht\", \"langweilig\", \"blöd\"]\n",
    "bar_neg = [\"schlecht\", \"fad\", \"bled\"]\n",
    "\n",
    "# parallel noun lists for data creation\n",
    "# picked from Editor suggestions in such a way that mostly all words are different\n",
    "eng_noun = [\"game\", \"site\", \"picture\", \"book\", \"story\", \"man\", \"world\", \"city\", \"time\", \"weather\", \"life\"]\n",
    "\n",
    "# standard German and Bavarian examples with determiners to avoid errors\n",
    "deu_noun = [\n",
    "    (\"Das\", \"Spiel\"), (\"Die\", \"Seite\"), (\"Das\", \"Bild\"), (\"Das\", \"Buch\"), (\"Die\", \"Geschichte\"), \n",
    "    (\"Der\", \"Mann\"), (\"Die\", \"Welt\"), (\"Die\", \"Stadt\"), (\"Die\", \"Zeit\"), (\"Das\", \"Wetter\"), (\"Das\", \"Leben\")\n",
    "    ]\n",
    "\n",
    "# bavarian determiners are with spaces to handle \"d'\" and \"s'\" determiners\n",
    "bar_noun = [\n",
    "    (\"Des \", \"Spui\"), (\"De \", \"Seitn\"), (\"Des \", \"Buidl\"), (\"Des \", \"Buach\"), (\"De \", \"Gschicht\"), \n",
    "    (\"Der \", \"Mo\"), (\"D'\", \"Weid\"), (\"D'\", \"Stod\"), (\"D'\", \"Zeid\"), (\"S'\", \"Weda\"), (\"S'\", \"Lebm\")\n",
    "    ] \n",
    "\n",
    "# negative phrases\n",
    "eng_neg_p = [\"I don't think that\", \"I hate that\", \"I don't like that\"]\n",
    "deu_neg_p = [\"Ich denke nicht, dass\", \"Ich hasse, dass\", \"Ich mag nicht, dass\"]\n",
    "bar_neg_p = [\"I deng ned, dass\", \"I hass des, wenn\", \"I mog des ned, wenn\"] # bavarian constructions work a bit differently than standard German\n",
    "\n",
    "# positive phrases\n",
    "eng_pos_p = [\"I like that\", \"I love that\", \"I'm sure that\"]\n",
    "deu_pos_p = [\"Ich mag, dass\", \"Ich liebe, dass\", \"Ich bin sicher, dass\"]\n",
    "bar_neg_p = [\"I mog des, wenn\", \"I liebs, wenn\", \"I bin ma sicha, dass\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English adj suggestions: \n",
      "['great', 'good', 'wonderful', 'beautiful', 'fantastic', 'terrible', 'fascinating', 'big', 'nice', 'bad', 'long', 'fun', 'scary', 'terrific', 'tough', 'short', 'serious', 'huge', 'remarkable', 'new'] \n",
      "\n",
      "German adj suggestions: \n",
      "['gutes', 'neues', 'großes', 'kleines', 'starkes', 'politisches', 'anderes', 'wichtiges', 'ganzes', 'besonderes', 'altes', 'deutsches', 'eigenes', 'super', 'weiteres', 'solches', 'zweites', 'einziges', 'letztes', 'erstes']\n",
      "['guter', 'schlechter', 'großer', 'schöner', 'super', 'wichtiger', 'deutscher', 'schwieriger', 'historischer', 'neuer', 'kleiner', 'politischer', 'typischer', 'anderer', 'besonderer', 'amerikanischer', 'klassischer', 'einfacher', 'internationaler', 'langer']\n"
     ]
    }
   ],
   "source": [
    "# get adj suggestions from the CheckList Editors in German and English\n",
    "print(\"English adj suggestions: \")\n",
    "print(eng_editor.suggest(\"This is a {mask} {noun}.\", noun=[\"book\", \"movie\"])[:20], \"\\n\") \n",
    "\n",
    "print(\"German adj suggestions: \")\n",
    "print(deu_editor.suggest(\"Das ist ein {mask} Buch.\")[:20]) \n",
    "print(deu_editor.suggest(\"Das ist ein {mask} Film.\")[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English noun suggestions: \n",
      "['game', 'one', 'stuff', 'book', 'article', 'video', 'post', 'movie', 'code', 'thing']\n",
      "['post', 'article', 'game', 'stuff', 'video', 'movie', 'list', 'shit', 'thing', 'book'] \n",
      "\n",
      "German noun suggestions: \n",
      "Der ['Preis', 'Job', 'Platz', 'Service', 'Abend', 'Wind', 'Rest', 'Sieg', 'Auftritt', 'Eindruck']\n",
      "Der ['Rest', 'Job', 'Film', 'Ton', 'Fußball', 'Mann', 'Preis', 'Markt', 'Anfang', 'Fall']\n",
      "Die ['Stimmung', 'Atmosphäre', 'Situation', 'Sache', 'Resonanz', 'Lage', 'Idee', 'Antwort', 'Qualität', 'Zusammenarbeit']\n",
      "Die ['Stimmung', 'Sache', 'Situation', 'Lage', 'Welt', 'Idee', 'Musik', 'Antwort', 'Geschichte', 'Zeit']\n",
      "Das ['Ergebnis', 'Wetter', 'alles', 'Leben', 'Angebot', 'hier', 'Spiel', 'Resultat', 'Stadion', 'Essen']\n",
      "Das ['Ergebnis', 'alles', 'Wetter', 'Leben', 'aber', 'Spiel', 'Ganze', 'hier', 'Auto', 'Geschäft']\n"
     ]
    }
   ],
   "source": [
    "# get noun suggestions from the CheckList Editors in German and English\n",
    "# english suggestions\n",
    "print(\"English noun suggestions: \")\n",
    "print(eng_editor.suggest(\"This {mask} is {pos}.\", pos=eng_pos)[:10]) \n",
    "print(eng_editor.suggest(\"This {mask} is {neg}.\", neg=eng_neg)[:10], \"\\n\") \n",
    "\n",
    "# suggestions for standard German with different determiners\n",
    "print(\"German noun suggestions: \")\n",
    "print(\"Der\", deu_editor.suggest(\"Der {mask} ist {pos}.\", pos=deu_pos)[:10])\n",
    "print(\"Der\", deu_editor.suggest(\"Der {mask} ist {neg}.\", neg=deu_neg)[:10])\n",
    "print(\"Die\", deu_editor.suggest(\"Die {mask} ist {pos}.\", pos=deu_pos)[:10])\n",
    "print(\"Die\", deu_editor.suggest(\"Die {mask} ist {neg}.\", neg=deu_neg)[:10])\n",
    "print(\"Das\", deu_editor.suggest(\"Das {mask} ist {pos}.\", pos=deu_pos)[:10])\n",
    "print(\"Das\", deu_editor.suggest(\"Das {mask} ist {neg}.\", neg=deu_neg)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity tests\n",
    "data_00 = eng_editor.template(\"The {noun} is {adj}.\", noun=eng_noun, adj=eng_pos, labels=2) # positive samples\n",
    "data_00 += eng_editor.template(\"The {noun} is {adj}.\", noun=eng_noun, adj=eng_neg, labels=0) # negative samples\n",
    "data_00_golds = data_00.labels\n",
    "test = MFT(**data_00)\n",
    "eng_suite.add(test, \"ENG: Sanity Check\", \"Negation\", \"Simple positive and negative sentences.\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English tests\n",
    "\n",
    "# test for negated positive examples, expectation: negative (0)\n",
    "# e.g. The game is not good.\n",
    "data01 = eng_editor.template(\"The {noun} is not {adj}.\", noun=eng_noun, adj=eng_pos, labels=0) # not pos = negative\n",
    "test = MFT(**data01)\n",
    "eng_suite.add(test, \"ENG: Positive Adjective Negations --> Negative: English\", \"Negation\", \"Sentences with negated positive adjectives.\", overwrite=True)\n",
    "\n",
    "# test for negations of positive phrases, expectation: negative (0)\n",
    "# e.g. I hate that the game is good.\n",
    "data02 = eng_editor.template(\"{p} the {noun} is {adj}.\", p=eng_neg_p, noun=eng_noun, adj=eng_pos, labels=0)\n",
    "test = MFT(**data02)\n",
    "eng_suite.add(test, \"ENG: Positive Phrase Negations --> Negative\", \"Negation\", \"Negations of positive sentences.\", overwrite=True)\n",
    "\n",
    "# test for negated negative adjectives, expectation: positive (1) (or 2)\n",
    "# e.g. The game is not bad.\n",
    "data03 = eng_editor.template(\"The {noun} is not {adj}.\", noun=eng_noun, adj=eng_pos, labels=2) # not neg = positive\n",
    "test = MFT(**data03)\n",
    "eng_suite.add(test, \"ENG: Negative Adjective Negations --> Positive\", \"Negation\", \"Sentences with negated negative adjectives.\", overwrite=True)\n",
    "\n",
    "# test for positive additional phrases to negative phrases, expectation: positive (1) (or 2)\n",
    "# e.g. I like that the game is bad.\n",
    "data04 = eng_editor.template(\"{p} the {noun} is {adj}.\", p=eng_pos_p, noun=eng_noun, adj=eng_neg, labels=2)\n",
    "test = MFT(**data04)\n",
    "eng_suite.add(test, \"ENG: Negative Phrases with Positive Additions --> Positive\", \"Negation\", \"Positive additions to negative sentences.\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "{0: 'negative', 1: 'neutral', 2: 'positive'}\n"
     ]
    }
   ],
   "source": [
    "# load multilingual sentiment analysis model\n",
    "model_name = \"clapAI/modernBERT-base-multilingual-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# initialise pipeline for predictions\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=device, top_k=None)\n",
    "\n",
    "idx2lbl = model.config.id2label\n",
    "lbl2idx = model.config.label2id\n",
    "print(lbl2idx)\n",
    "print(idx2lbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CheckList Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for prediction and formatting\n",
    "\n",
    "def predict_to_file(data, preds):\n",
    "    # read data and predict\n",
    "    with open(data, \"r\") as f:\n",
    "        data = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    raw_preds = pipe(data)\n",
    "\n",
    "    # write results in correct CheckList format to a file\n",
    "    with open(preds, \"w\") as f:\n",
    "        f.write(\"prediction positive neutral negative\\n\")\n",
    "        for result in raw_preds:\n",
    "            positive = result[0]\n",
    "            neutral = result[1]\n",
    "            negative = result[2]\n",
    "\n",
    "            max_pred = max([negative, positive, neutral], key=lambda x: x[\"score\"])\n",
    "            max_label = max_pred[\"label\"]\n",
    "\n",
    "            f.write(f\"{idx2lbl[max_label]} {positive[\"score\"]} {neutral[\"score\"]} {negative[\"score\"]} \\n\")\n",
    "\n",
    "def format_example(x, pred, conf, label, meta=None): \n",
    "    return f\"Sentence: {x}\\nGold: {label}\\t\\tPredicted: {pred}\\np(positive) = {round(conf[0], 2)}\\tp(neutral) = {round(conf[1], 2)}\\tp(negative) = {round(conf[2], 2)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_data_file = \"/tmp/eng_data.txt\"\n",
    "eng_predictions_file = \"/tmp/eng_predictions.txt\"\n",
    "\n",
    "eng_suite.to_raw_file(eng_data_file)\n",
    "predict_to_file(eng_data_file, eng_predictions_file)\n",
    "\n",
    "eng_suite.run_from_file(eng_predictions_file, file_format=\"pred_and_softmax\", ignore_header=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negation\n",
      "\n",
      "Sanity Check\n",
      "Language: English\n",
      "Test cases:      66\n",
      "Fails (rate):    3 (4.5%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The weather is bad.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.57\tp(neutral) = 0.42\tp(negative) = 0.01\n",
      "----\n",
      "Sentence: The weather is boring.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.51\tp(neutral) = 0.48\tp(negative) = 0.01\n",
      "----\n",
      "Sentence: The weather is good.\n",
      "Gold: 2\t\tPredicted: 1\n",
      "p(positive) = 0.49\tp(neutral) = 0.47\tp(negative) = 0.04\n",
      "----\n",
      "\n",
      "\n",
      "Positive Adjective Negations: Negative\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    3 (9.1%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The weather is not nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.66\tp(neutral) = 0.32\tp(negative) = 0.02\n",
      "----\n",
      "Sentence: The weather is not great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.74\tp(neutral) = 0.24\tp(negative) = 0.02\n",
      "----\n",
      "Sentence: The weather is not good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.63\tp(neutral) = 0.36\tp(negative) = 0.01\n",
      "----\n",
      "\n",
      "\n",
      "Positive Phrase Negations: Negative\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    5 (5.1%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I hate that the weather is great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.53\tp(neutral) = 0.33\tp(negative) = 0.14\n",
      "----\n",
      "Sentence: I don't think that the weather is good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.54\tp(neutral) = 0.45\tp(negative) = 0.01\n",
      "----\n",
      "Sentence: I hate that the weather is good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.55\tp(neutral) = 0.37\tp(negative) = 0.08\n",
      "----\n",
      "\n",
      "\n",
      "Negative Adjective Negations: Positive\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    33 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The time is not nice.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.75\tp(neutral) = 0.18\tp(negative) = 0.07\n",
      "----\n",
      "Sentence: The world is not nice.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.81\tp(neutral) = 0.15\tp(negative) = 0.04\n",
      "----\n",
      "Sentence: The picture is not nice.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.74\tp(neutral) = 0.17\tp(negative) = 0.09\n",
      "----\n",
      "\n",
      "\n",
      "Negative Phrases with Positive Additions: Positive\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    83 (83.8%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I like that the site is stupid.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.73\tp(neutral) = 0.23\tp(negative) = 0.04\n",
      "----\n",
      "Sentence: I'm sure that the book is boring.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.93\tp(neutral) = 0.04\tp(negative) = 0.04\n",
      "----\n",
      "Sentence: I like that the book is boring.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.69\tp(neutral) = 0.17\tp(negative) = 0.14\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eng_suite.summary(format_example_fn = format_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]]\n",
      "[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 1, 1, 0, 2, 2, 2, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0]]\n",
      "{'Sanity Check\\nLanguage: English': {'negative': 0, 'neutral': 0, 'positive': 0}, 'Positive Adjective Negations: Negative\\nLanguage: English': {'negative': 0, 'neutral': 0, 'positive': 0}, 'Positive Phrase Negations: Negative\\nLanguage: English': {'negative': 0, 'neutral': 0, 'positive': 0}, 'Negative Adjective Negations: Positive\\nLanguage: English': {'negative': 0, 'neutral': 0, 'positive': 0}, 'Negative Phrases with Positive Additions: Positive\\nLanguage: English': {'negative': 0, 'neutral': 0, 'positive': 0}}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 86\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;66;03m# plt.tight_layout()\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 86\u001b[0m \u001b[43mvisualise\u001b[49m\u001b[43m(\u001b[49m\u001b[43meng_suite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meng_predictions_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[44], line 38\u001b[0m, in \u001b[0;36mvisualise\u001b[0;34m(testsuite, predictions_file)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m classes:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, n_tests):\n\u001b[0;32m---> 38\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m gold, pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgold_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_labels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m gold \u001b[38;5;241m==\u001b[39m label \u001b[38;5;129;01mand\u001b[39;00m gold \u001b[38;5;241m==\u001b[39m pred: \u001b[38;5;66;03m# if gold is the current label and a correct prediction\u001b[39;00m\n\u001b[1;32m     40\u001b[0m                 correct_preds_per_label[label] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "def visualise(testsuite, predictions_file):\n",
    "    # read prediction scores\n",
    "    with open(predictions_file, \"r\") as f:\n",
    "        all_predictions = [line.split() for line in f][1:] # remove header line\n",
    "\n",
    "    pred_labels = [int(item[0]) for item in all_predictions]\n",
    "    pred_scores = [item[1:] for item in all_predictions]\n",
    "\n",
    "    # get gold labels\n",
    "    gold_labels = [test.labels for test in testsuite.tests.values()]\n",
    "    n_tests = len(gold_labels)\n",
    "\n",
    "    # order predicted labels into tests\n",
    "    ordered_pred_labels = []\n",
    "    ordered_pred_scores = []\n",
    "    start = 0\n",
    "\n",
    "    for test in gold_labels:\n",
    "        length = len(test)\n",
    "        ordered_pred_labels.append(pred_labels[start:start+length])\n",
    "        ordered_pred_scores.append(pred_scores[start:start+length])\n",
    "        start += length\n",
    "\n",
    "    print(gold_labels)\n",
    "    print(ordered_pred_labels)\n",
    "\n",
    "    # label information\n",
    "    classes = lbl2idx.keys()\n",
    "    num_classes = np.arange(len(classes)) # spaced out for pyplot\n",
    "\n",
    "    # extract counts and calculate percentages of correct and false predictions\n",
    "    correct_preds_per_label = {test:{label:0 for label in classes} for test in testsuite.tests}\n",
    "    false_preds_per_label = {test:{label:0 for label in classes} for test in testsuite.tests}\n",
    "\n",
    "    for label in classes:\n",
    "        for i in range(0, n_tests):\n",
    "            for gold, pred in zip(gold_labels[i], pred_labels[i]):\n",
    "                if gold == lbl2idx[label] and gold == pred: # if gold is the current label and a correct prediction\n",
    "                    correct_preds_per_label[label] += 1\n",
    "\n",
    "                else: \n",
    "                    false_preds_per_label[label] += 1\n",
    "\n",
    "\n",
    "    # TODO\n",
    "    # open figure with n_test subplots\n",
    "    # for test in tests, fill subplot with plot\n",
    "\n",
    "    \n",
    "    return None\n",
    "    # from integer labels to text labels\n",
    "    gold_labels = [idx2lbl[label] for label in golds]\n",
    "    pred_labels = [idx2lbl[label] for label in preds]\n",
    "    \n",
    "    classes = lbl2idx.keys()\n",
    "    num_classes = np.arange(len(lbl2idx)) # spaced out for pyplot\n",
    "\n",
    "    # extract counts and calculate percentages of correct and false predictions\n",
    "    correct_preds_per_label = {label:0 for label in classes}\n",
    "    false_preds_per_label = {label:0 for label in classes}\n",
    "\n",
    "    for label in classes:\n",
    "        for gold, pred in zip(gold_labels, pred_labels):\n",
    "            if gold == label and gold == pred: # if gold is the current label and a correct prediction\n",
    "                correct_preds_per_label[label] += 1\n",
    "\n",
    "            else: \n",
    "                false_preds_per_label[label] += 1\n",
    "\n",
    "    correct_percentages = [(count / len(gold_labels)) * 100 for count in correct_preds_per_label.values()]\n",
    "    false_percentages = [(count / len(gold_labels)) * 100 for count in false_preds_per_label.values()]\n",
    "\n",
    "    # design bar chart\n",
    "    width = 0.8\n",
    "    plt.bar(num_classes, correct_percentages, width, label = \"Correct\", color = \"mediumseagreen\")\n",
    "    plt.bar(num_classes, false_percentages, width, bottom=correct_percentages, label = \"False\", color = \"salmon\")\n",
    "\n",
    "    plt.xlabel(\"Labels\")\n",
    "    plt.ylabel(\"Percentage\")\n",
    "    plt.title(\"Correct and False Predictions per Label\")\n",
    "    plt.xticks(num_classes, classes)\n",
    "    plt.legend()\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualise(eng_suite, eng_predictions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trustenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
