{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes\n",
    "# running the suite with a prediction function does not allow for access to the scores, so if you need the scores themselves, you have to predict to a file\n",
    "# and run the suite from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.editor import Editor\n",
    "from checklist.test_types import MFT\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise CheckList Editor objects\n",
    "eng_editor = Editor() # default language = English\n",
    "deu_editor = Editor(language=\"german\")\n",
    "\n",
    "# initialise CheckList TestSuite for running the tests\n",
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linguistic resources\n",
    "\n",
    "# parallel adjective lists for data creation\n",
    "eng_pos = [\"good\", \"nice\", \"great\"]\n",
    "deu_pos = [\"gut\", \"schön\", \"super\"]\n",
    "bar_pos = [\"guad\", \"schee\", \"subba\"]\n",
    "\n",
    "eng_neg = [\"bad\", \"boring\", \"stupid\"]\n",
    "deu_neg = [\"schlecht\", \"langweilig\", \"blöd\"]\n",
    "bar_neg = [\"schlecht\", \"fad\", \"bled\"]\n",
    "\n",
    "# parallel noun lists for data creation\n",
    "# picked from Editor suggestions in such a way that mostly all words are different\n",
    "eng_noun = [\"game\", \"site\", \"picture\", \"book\", \"story\", \"man\", \"world\", \"city\", \"time\", \"weather\", \"life\"]\n",
    "\n",
    "# standard German and Bavarian examples with determiners to avoid errors\n",
    "deu_noun = [\n",
    "    (\"Das\", \"Spiel\"), (\"Die\", \"Seite\"), (\"Das\", \"Bild\"), (\"Das\", \"Buch\"), (\"Die\", \"Geschichte\"), \n",
    "    (\"Der\", \"Mann\"), (\"Die\", \"Welt\"), (\"Die\", \"Stadt\"), (\"Die\", \"Zeit\"), (\"Das\", \"Wetter\"), (\"Das\", \"Leben\")\n",
    "    ]\n",
    "\n",
    "# bavarian determiners are with spaces to handle \"d'\" and \"s'\" determiners\n",
    "bar_noun = [\n",
    "    (\"Des \", \"Spui\"), (\"De \", \"Seitn\"), (\"Des \", \"Buidl\"), (\"Des \", \"Buach\"), (\"De \", \"Gschicht\"), \n",
    "    (\"Der \", \"Mo\"), (\"D'\", \"Weid\"), (\"D'\", \"Stod\"), (\"D'\", \"Zeid\"), (\"S'\", \"Weda\"), (\"S'\", \"Lebm\")\n",
    "    ] \n",
    "\n",
    "# negative phrases\n",
    "eng_neg_p = [\"I don't think that\", \"I hate that\", \"I don't like that\"]\n",
    "deu_neg_p = [\"Ich denke nicht, dass\", \"Ich hasse, dass\", \"Ich mag nicht, dass\"]\n",
    "bar_neg_p = [\"I deng ned, dass\", \"I hass des, wenn\", \"I mog des ned, wenn\"] # bavarian constructions work a bit differently than standard German\n",
    "\n",
    "# positive phrases\n",
    "eng_pos_p = [\"I like that\", \"I love that\", \"I'm sure that\"]\n",
    "deu_pos_p = [\"Ich mag, dass\", \"Ich liebe, dass\", \"Ich bin sicher, dass\"]\n",
    "bar_neg_p = [\"I mog des, wenn\", \"I liebs, wenn\", \"I bin ma sicha, dass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity tests\n",
    "data = eng_editor.template(\"The {noun} is {adj}.\", noun=eng_noun, adj=eng_pos, labels=1) # positive samples\n",
    "data += eng_editor.template(\"The {noun} is {adj}.\", noun=eng_noun, adj=eng_neg, labels=0) # negative samples\n",
    "test = MFT(**data)\n",
    "suite.add(test, \"Sanity Check\\nLanguage: English\", \"Negation\", \"Simple positive and negative sentences.\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English tests\n",
    "\n",
    "# test for negated positive examples, expectation: negative (0)\n",
    "data = eng_editor.template(\"The {noun} is not {adj}.\", noun=eng_noun, adj=eng_pos, labels=0) # not pos = negative\n",
    "test = MFT(data.data, labels=0)\n",
    "suite.add(test, \"Positive Adjective Negations: Negative\\nLanguage: English\", \"Negation\", \"Sentences with negated positive adjectives.\", overwrite=True)\n",
    "\n",
    "# test for negations of positive phrases, expectation: negative (0)\n",
    "data = eng_editor.template(\"{p} the {noun} is {adj}.\", p=eng_neg_p, noun=eng_noun, adj=eng_pos, labels=0)\n",
    "test = MFT(data.data, labels=0)\n",
    "suite.add(test, \"Positive Phrase Negations: Negative\\nLanguage: English\", \"Negation\", \"Negations of positive sentences.\", overwrite=True)\n",
    "\n",
    "# test for negated negative adjectives, expectation: positive (1) (for SentiMBERT 2)\n",
    "data = eng_editor.template(\"The {noun} is not {adj}.\", noun=eng_noun, adj=eng_pos, labels=1) # not neg = positive\n",
    "test = MFT(data.data, labels=1)\n",
    "suite.add(test, \"Negative Adjective Negations: Positive\\nLanguage: English\", \"Negation\", \"Sentences with negated negative adjectives.\", overwrite=True)\n",
    "\n",
    "# test for positive additional phrases to negative phrases, expectation: positive (1) (for SentiMBERT 2)\n",
    "data = eng_editor.template(\"{p} the {noun} is {adj}.\", p=eng_pos_p, noun=eng_noun, adj=eng_neg, labels=1)\n",
    "test = MFT(data.data, labels=1)\n",
    "suite.add(test, \"Negative Phrases with Positive Additions: Positive\\nLanguage: English\", \"Negation\", \"Positive additions to negative sentences.\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# load vanilla mBERT model\n",
    "model_name = \"google-bert/bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# initialise pipeline for predictions\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=device, top_k=None)\n",
    "\n",
    "lbl2idx = {\"LABEL_0\": 0, \"LABEL_1\": 1}\n",
    "idx2lbl = {0: \"LABEL_0\", 1: \"LABEL_1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    # read data and predict\n",
    "    raw_preds = pipe(data)\n",
    "\n",
    "    preds = []\n",
    "    confs = []\n",
    "\n",
    "    # write results in correct CheckList format to a file\n",
    "    for result in raw_preds:\n",
    "        negative = result[0]\n",
    "        positive = result[1]\n",
    "\n",
    "        max_pred = max([negative, positive], key=lambda x: x[\"score\"])\n",
    "        max_label = max_pred[\"label\"]\n",
    "\n",
    "        # prediction, negative_score, neutral_score, positive_score\n",
    "        preds.append(lbl2idx[max_label])\n",
    "        confs.append(np.array([negative[\"score\"], positive[\"score\"]]))\n",
    "\n",
    "    return preds, confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Positive Adjective Negations: Negative\n",
      "Language: English\n",
      "Predicting 33 examples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Positive Phrase Negations: Negative\n",
      "Language: English\n",
      "Predicting 99 examples\n",
      "Running Negative Adjective Negations: Positive\n",
      "Language: English\n",
      "Predicting 33 examples\n",
      "Running Negative Phrases with Positive Additions: Positive\n",
      "Language: English\n",
      "Predicting 99 examples\n",
      "Running Sanity Check\n",
      "Language: English\n",
      "Predicting 66 examples\n"
     ]
    }
   ],
   "source": [
    "suite.run(predict, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(x, pred, conf, label, meta=None): \n",
    "    return f\"Sentence: {x}\\nGold: {pred}\\t\\tPredicted: {label}\\np(negative) = {round(conf[0], 2)}\\tp(positive) = {round(conf[1], 2)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negation\n",
      "\n",
      "Positive Adjective Negations: Negative\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    11 (33.3%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The game is not great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "Sentence: The man is not nice.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: The time is not great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "Positive Phrase Negations: Negative\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    52 (52.5%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I don't think that the weather is great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: I hate that the story is great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "Sentence: I hate that the weather is great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "\n",
      "\n",
      "Negative Adjective Negations: Positive\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    22 (66.7%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The city is not great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: The book is not great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: The city is not good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.53\tp(positive) = 0.47\n",
      "----\n",
      "\n",
      "\n",
      "Negative Phrases with Positive Additions: Positive\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    39 (39.4%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I like that the game is bad.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: I like that the picture is bad.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "Sentence: I'm sure that the story is bad.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "Sanity Check\n",
      "Language: English\n",
      "Test cases:      66\n",
      "Fails (rate):    29 (43.9%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The game is boring.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "Sentence: The weather is good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.54\tp(positive) = 0.46\n",
      "----\n",
      "Sentence: The picture is great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary(format_example_fn = format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_to_file(data):\n",
    "    # read data and predict\n",
    "    with open(data, \"r\") as f:\n",
    "        data = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    raw_preds = pipe(data)\n",
    "\n",
    "    # write results in correct CheckList format to a file\n",
    "    with open(\"/tmp/predictions.txt\", \"w\") as f:\n",
    "        f.write(\"label negative positive\\n\")\n",
    "        for result in raw_preds:\n",
    "            negative = result[0]\n",
    "            positive = result[1]\n",
    "\n",
    "            max_pred = max([negative, positive], key=lambda x: x[\"score\"])\n",
    "            max_label = max_pred[\"label\"]\n",
    "\n",
    "            f.write(f\"{lbl2idx[max_label]} {negative[\"score\"]} {positive[\"score\"]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.to_raw_file(\"/tmp/data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_to_file(\"/tmp/data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.run_from_file(\"/tmp/predictions.txt\", file_format=\"pred_and_softmax\", ignore_header=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negation\n",
      "\n",
      "Positive Adjective Negations: Negative\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    11 (33.3%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The life is not nice.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "Sentence: The world is not great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: The world is not good.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "\n",
      "\n",
      "Positive Phrase Negations: Negative\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    52 (52.5%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I don't think that the world is great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "Sentence: I don't think that the life is nice.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.53\tp(positive) = 0.47\n",
      "----\n",
      "Sentence: I don't like that the time is good.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "Negative Adjective Negations: Positive\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    22 (66.7%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The weather is not good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.54\tp(positive) = 0.46\n",
      "----\n",
      "Sentence: The city is not nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "Sentence: The site is not nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "\n",
      "\n",
      "Negative Phrases with Positive Additions: Positive\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    39 (39.4%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I like that the story is boring.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "Sentence: I like that the game is boring.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: I love that the city is boring.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "Sanity Check\n",
      "Language: English\n",
      "Test cases:      66\n",
      "Fails (rate):    29 (43.9%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The story is great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "Sentence: The site is nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "Sentence: The picture is nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary(format_example_fn = format_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trustenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
