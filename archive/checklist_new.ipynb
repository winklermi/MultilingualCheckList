{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes\n",
    "# running the suite with a prediction function does not allow for access to the scores, so if you need the scores themselves, you have to predict to a file\n",
    "# and run the suite from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from checklist.editor import Editor\n",
    "from checklist.test_types import MFT\n",
    "from checklist.test_suite import TestSuite\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise CheckList Editor objects\n",
    "eng_editor = Editor() # default language = English\n",
    "deu_editor = Editor(language=\"german\")\n",
    "\n",
    "# initialise CheckList TestSuite for running the tests\n",
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linguistic resources\n",
    "\n",
    "# parallel adjective lists for data creation\n",
    "eng_pos = [\"good\", \"nice\", \"great\"]\n",
    "deu_pos = [\"gut\", \"schön\", \"super\"]\n",
    "bar_pos = [\"guad\", \"schee\", \"subba\"]\n",
    "\n",
    "eng_neg = [\"bad\", \"boring\", \"stupid\"]\n",
    "deu_neg = [\"schlecht\", \"langweilig\", \"blöd\"]\n",
    "bar_neg = [\"schlecht\", \"fad\", \"bled\"]\n",
    "\n",
    "# parallel noun lists for data creation\n",
    "# picked from Editor suggestions in such a way that mostly all words are different\n",
    "eng_noun = [\"game\", \"site\", \"picture\", \"book\", \"story\", \"man\", \"world\", \"city\", \"time\", \"weather\", \"life\"]\n",
    "\n",
    "# standard German and Bavarian examples with determiners to avoid errors\n",
    "deu_noun = [\n",
    "    (\"Das\", \"Spiel\"), (\"Die\", \"Seite\"), (\"Das\", \"Bild\"), (\"Das\", \"Buch\"), (\"Die\", \"Geschichte\"), \n",
    "    (\"Der\", \"Mann\"), (\"Die\", \"Welt\"), (\"Die\", \"Stadt\"), (\"Die\", \"Zeit\"), (\"Das\", \"Wetter\"), (\"Das\", \"Leben\")\n",
    "    ]\n",
    "\n",
    "# bavarian determiners are with spaces to handle \"d'\" and \"s'\" determiners\n",
    "bar_noun = [\n",
    "    (\"Des \", \"Spui\"), (\"De \", \"Seitn\"), (\"Des \", \"Buidl\"), (\"Des \", \"Buach\"), (\"De \", \"Gschicht\"), \n",
    "    (\"Der \", \"Mo\"), (\"D'\", \"Weid\"), (\"D'\", \"Stod\"), (\"D'\", \"Zeid\"), (\"S'\", \"Weda\"), (\"S'\", \"Lebm\")\n",
    "    ] \n",
    "\n",
    "# negative phrases\n",
    "eng_neg_p = [\"I don't think that\", \"I hate that\", \"I don't like that\"]\n",
    "deu_neg_p = [\"Ich denke nicht, dass\", \"Ich hasse, dass\", \"Ich mag nicht, dass\"]\n",
    "bar_neg_p = [\"I deng ned, dass\", \"I hass des, wenn\", \"I mog des ned, wenn\"] # bavarian constructions work a bit differently than standard German\n",
    "\n",
    "# positive phrases\n",
    "eng_pos_p = [\"I like that\", \"I love that\", \"I'm sure that\"]\n",
    "deu_pos_p = [\"Ich mag, dass\", \"Ich liebe, dass\", \"Ich bin sicher, dass\"]\n",
    "bar_neg_p = [\"I mog des, wenn\", \"I liebs, wenn\", \"I bin ma sicha, dass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity tests - classify sentences\n",
    "data = eng_editor.template(\"The {noun} is {adj}.\", noun=eng_noun, adj=eng_pos, labels=1) # positive samples\n",
    "data += eng_editor.template(\"The {noun} is {adj}.\", noun=eng_noun, adj=eng_neg, labels=0) # negative samples\n",
    "test = MFT(**data)\n",
    "suite.add(test, \"Sanity Checks\\nLanguage: English\", \"Negation\", \"Simple positive and negative sentences.\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English tests\n",
    "\n",
    "# test for negated positive examples, expectation: negative (0)\n",
    "# e.g. The game is not good.\n",
    "data01 = eng_editor.template(\"The {noun} is not {adj}.\", noun=eng_noun, adj=eng_pos, labels=0) # not pos = negative\n",
    "test = MFT(**data01)\n",
    "suite.add(test, \"Positive Adjective Negations: Negative\\nLanguage: English\", \"Negation\", \"Sentences with negated positive adjectives.\", overwrite=True)\n",
    "\n",
    "# test for negations of positive phrases, expectation: negative (0)\n",
    "# e.g. I hate that the game is good.\n",
    "data02 = eng_editor.template(\"{p} the {noun} is {adj}.\", p=eng_neg_p, noun=eng_noun, adj=eng_pos, labels=0)\n",
    "test = MFT(**data02)\n",
    "suite.add(test, \"Positive Phrase Negations: Negative\\nLanguage: English\", \"Negation\", \"Negations of positive sentences.\", overwrite=True)\n",
    "\n",
    "# test for negated negative adjectives, expectation: positive (1) (or 2)\n",
    "# e.g. The game is not bad.\n",
    "data03 = eng_editor.template(\"The {noun} is not {adj}.\", noun=eng_noun, adj=eng_pos, labels=1) # not neg = positive\n",
    "test = MFT(**data03)\n",
    "suite.add(test, \"Negative Adjective Negations: Positive\\nLanguage: English\", \"Negation\", \"Sentences with negated negative adjectives.\", overwrite=True)\n",
    "\n",
    "# test for positive additional phrases to negative phrases, expectation: positive (1) (or 2)\n",
    "# e.g. I like that the game is bad.\n",
    "data04 = eng_editor.template(\"{p} the {noun} is {adj}.\", p=eng_pos_p, noun=eng_noun, adj=eng_neg, labels=1)\n",
    "test = MFT(**data04)\n",
    "suite.add(test, \"Negative Phrases with Positive Additions: Positive\\nLanguage: English\", \"Negation\", \"Positive additions to negative sentences.\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# load vanilla mBERT model\n",
    "model_name = \"google-bert/bert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# initialise pipeline for predictions\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=device, top_k=None)\n",
    "\n",
    "lbl2idx = {\"LABEL_0\": 0, \"LABEL_1\": 1}\n",
    "idx2lbl = {0: \"LABEL_0\", 1: \"LABEL_1\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data):\n",
    "    # read data and predict\n",
    "    raw_preds = pipe(data)\n",
    "\n",
    "    preds = []\n",
    "    confs = []\n",
    "\n",
    "    # write results in correct CheckList format to a file\n",
    "    for result in raw_preds:\n",
    "        negative = result[0]\n",
    "        positive = result[1]\n",
    "\n",
    "        max_pred = max([negative, positive], key=lambda x: x[\"score\"])\n",
    "        max_label = max_pred[\"label\"]\n",
    "\n",
    "        # prediction, negative_score, neutral_score, positive_score\n",
    "        preds.append(lbl2idx[max_label])\n",
    "        confs.append(np.array([negative[\"score\"], positive[\"score\"]]))\n",
    "\n",
    "    return preds, confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negation\n",
      "\n",
      "Positive Adjective Negations: Negative\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    11 (33.3%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The game is not great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "Sentence: The man is not nice.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: The time is not great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "Positive Phrase Negations: Negative\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    52 (52.5%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I don't think that the weather is great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: I hate that the story is great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "Sentence: I hate that the weather is great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "\n",
      "\n",
      "Negative Adjective Negations: Positive\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    22 (66.7%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The city is not great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: The book is not great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: The city is not good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.53\tp(positive) = 0.47\n",
      "----\n",
      "\n",
      "\n",
      "Negative Phrases with Positive Additions: Positive\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    39 (39.4%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I like that the game is bad.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: I like that the picture is bad.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "Sentence: I'm sure that the story is bad.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "Sanity Check\n",
      "Language: English\n",
      "Test cases:      66\n",
      "Fails (rate):    29 (43.9%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The game is boring.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "Sentence: The weather is good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.54\tp(positive) = 0.46\n",
      "----\n",
      "Sentence: The picture is great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.run(predict, overwrite=True)\n",
    "\n",
    "def format_example(x, pred, conf, label, meta=None): \n",
    "    return f\"Sentence: {x}\\nGold: {pred}\\t\\tPredicted: {label}\\np(negative) = {round(conf[0], 2)}\\tp(positive) = {round(conf[1], 2)}\"\n",
    "\n",
    "suite.summary(format_example_fn = format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_to_file(data, preds):\n",
    "    # read data and predict\n",
    "    with open(data, \"r\") as f:\n",
    "        data = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    raw_preds = pipe(data)\n",
    "\n",
    "    # write results in correct CheckList format to a file\n",
    "    with open(preds, \"w\") as f:\n",
    "        f.write(\"label negative positive\\n\")\n",
    "        for result in raw_preds:\n",
    "            negative = result[0]\n",
    "            positive = result[1]\n",
    "\n",
    "            max_pred = max([negative, positive], key=lambda x: x[\"score\"])\n",
    "            max_label = max_pred[\"label\"]\n",
    "\n",
    "            f.write(f\"{lbl2idx[max_label]} {negative[\"score\"]} {positive[\"score\"]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/tmp/data.txt\"\n",
    "predictions_file = \"/tmp/predictions.txt\"\n",
    "\n",
    "suite.to_raw_file(data_file)\n",
    "predict_to_file(data_file, predictions_file)\n",
    "\n",
    "suite.run_from_file(predictions_file, file_format=\"pred_and_softmax\", ignore_header=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negation\n",
      "\n",
      "Positive Adjective Negations: Negative\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    11 (33.3%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The life is not nice.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "Sentence: The world is not great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: The world is not good.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "\n",
      "\n",
      "Positive Phrase Negations: Negative\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    52 (52.5%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I don't think that the world is great.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "Sentence: I don't think that the life is nice.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.53\tp(positive) = 0.47\n",
      "----\n",
      "Sentence: I don't like that the time is good.\n",
      "Gold: 1\t\tPredicted: 0\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "Negative Adjective Negations: Positive\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    22 (66.7%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The weather is not good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.54\tp(positive) = 0.46\n",
      "----\n",
      "Sentence: The city is not nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "Sentence: The site is not nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "\n",
      "\n",
      "Negative Phrases with Positive Additions: Positive\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    39 (39.4%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I like that the story is boring.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "Sentence: I like that the game is boring.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "Sentence: I love that the city is boring.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "Sanity Check\n",
      "Language: English\n",
      "Test cases:      66\n",
      "Fails (rate):    29 (43.9%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The story is great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.5\tp(positive) = 0.5\n",
      "----\n",
      "Sentence: The site is nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.52\tp(positive) = 0.48\n",
      "----\n",
      "Sentence: The picture is nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(negative) = 0.51\tp(positive) = 0.49\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "suite.summary(format_example_fn = format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"git+https://github.com/huggingface/transformers.git@6e0515e99c39444caae39472ee1b2fd76ece32f1\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modernbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'negative', 1: 'neutral', 2: 'positive'}\n",
      "{'negative': 0, 'neutral': 1, 'positive': 2}\n"
     ]
    }
   ],
   "source": [
    "# load multilingual sentiment analysis model\n",
    "model_name = \"clapAI/modernBERT-base-multilingual-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# initialise pipeline for predictions\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, device=device, top_k=None)\n",
    "\n",
    "lbl2idx = model.config.id2label\n",
    "idx2lbl = model.config.label2id\n",
    "print(lbl2idx)\n",
    "print(idx2lbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity tests\n",
    "data = eng_editor.template(\"The {noun} is {adj}.\", noun=eng_noun, adj=eng_pos, labels=2) # positive samples\n",
    "data += eng_editor.template(\"The {noun} is {adj}.\", noun=eng_noun, adj=eng_neg, labels=0) # negative samples\n",
    "test = MFT(**data)\n",
    "suite.add(test, \"Sanity Check\\nLanguage: English\", \"Negation\", \"Simple positive and negative sentences.\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# English tests\n",
    "\n",
    "# test for negated positive examples, expectation: negative (0)\n",
    "# e.g. The game is not good.\n",
    "data01 = eng_editor.template(\"The {noun} is not {adj}.\", noun=eng_noun, adj=eng_pos, labels=0) # not pos = negative\n",
    "test = MFT(**data01)\n",
    "suite.add(test, \"Positive Adjective Negations: Negative\\nLanguage: English\", \"Negation\", \"Sentences with negated positive adjectives.\", overwrite=True)\n",
    "\n",
    "# test for negations of positive phrases, expectation: negative (0)\n",
    "# e.g. I hate that the game is good.\n",
    "data02 = eng_editor.template(\"{p} the {noun} is {adj}.\", p=eng_neg_p, noun=eng_noun, adj=eng_pos, labels=0)\n",
    "test = MFT(**data02)\n",
    "suite.add(test, \"Positive Phrase Negations: Negative\\nLanguage: English\", \"Negation\", \"Negations of positive sentences.\", overwrite=True)\n",
    "\n",
    "# test for negated negative adjectives, expectation: positive (1) (or 2)\n",
    "# e.g. The game is not bad.\n",
    "data03 = eng_editor.template(\"The {noun} is not {adj}.\", noun=eng_noun, adj=eng_pos, labels=2) # not neg = positive\n",
    "test = MFT(**data03)\n",
    "suite.add(test, \"Negative Adjective Negations: Positive\\nLanguage: English\", \"Negation\", \"Sentences with negated negative adjectives.\", overwrite=True)\n",
    "\n",
    "# test for positive additional phrases to negative phrases, expectation: positive (1) (or 2)\n",
    "# e.g. I like that the game is bad.\n",
    "data04 = eng_editor.template(\"{p} the {noun} is {adj}.\", p=eng_pos_p, noun=eng_noun, adj=eng_neg, labels=2)\n",
    "test = MFT(**data04)\n",
    "suite.add(test, \"Negative Phrases with Positive Additions: Positive\\nLanguage: English\", \"Negation\", \"Positive additions to negative sentences.\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_to_file(data, preds):\n",
    "    # read data and predict\n",
    "    with open(data, \"r\") as f:\n",
    "        data = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    raw_preds = pipe(data)\n",
    "\n",
    "    # write results in correct CheckList format to a file\n",
    "    with open(preds, \"w\") as f:\n",
    "        f.write(\"prediction positive neutral negative\\n\")\n",
    "        for result in raw_preds:\n",
    "            positive = result[0]\n",
    "            neutral = result[1]\n",
    "            negative = result[2]\n",
    "\n",
    "            max_pred = max([negative, positive, neutral], key=lambda x: x[\"score\"])\n",
    "            max_label = max_pred[\"label\"]\n",
    "\n",
    "            f.write(f\"{idx2lbl[max_label]} {positive[\"score\"]} {neutral[\"score\"]} {negative[\"score\"]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'positive', 'score': 0.7783976197242737}, {'label': 'negative', 'score': 0.11116747558116913}, {'label': 'neutral', 'score': 0.11043490469455719}]\n"
     ]
    }
   ],
   "source": [
    "data_file = \"/tmp/data.txt\"\n",
    "predictions_file = \"/tmp/predictions.txt\"\n",
    "\n",
    "suite.to_raw_file(data_file)\n",
    "predict_to_file(data_file, predictions_file)\n",
    "\n",
    "suite.run_from_file(predictions_file, file_format=\"pred_and_softmax\", ignore_header=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction positive neutral negative\n",
      "2 0.7783976197242737 0.11116747558116913 0.11043490469455719 \n",
      "2 0.539217472076416 0.3811604678630829 0.07962208241224289 \n",
      "2 0.9282175898551941 0.0496378131210804 0.022144557908177376 \n",
      "2 0.8421204686164856 0.10394899547100067 0.053930483758449554 \n",
      "2 0.4523354470729828 0.40778809785842896 0.13987642526626587 \n",
      "2 0.9213922023773193 0.05033658817410469 0.028271200135350227 \n",
      "2 0.5943126082420349 0.29849109053611755 0.10719628632068634 \n",
      "2 0.6096277832984924 0.2928953170776367 0.09747689217329025 \n",
      "2 0.8210318684577942 0.09937994182109833 0.07958824932575226 \n"
     ]
    }
   ],
   "source": [
    "!cat /tmp/predictions.txt | head -10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negation\n",
      "\n",
      "Sanity Check\n",
      "Language: English\n",
      "Test cases:      66\n",
      "Fails (rate):    3 (4.5%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The weather is bad.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.57\tp(neutral) = 0.42\tp(negative) = 0.01\n",
      "----\n",
      "Sentence: The weather is boring.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.51\tp(neutral) = 0.48\tp(negative) = 0.01\n",
      "----\n",
      "Sentence: The weather is good.\n",
      "Gold: 2\t\tPredicted: 1\n",
      "p(positive) = 0.49\tp(neutral) = 0.47\tp(negative) = 0.04\n",
      "----\n",
      "\n",
      "\n",
      "Positive Adjective Negations: Negative\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    3 (9.1%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The weather is not good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.63\tp(neutral) = 0.36\tp(negative) = 0.01\n",
      "----\n",
      "Sentence: The weather is not nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.66\tp(neutral) = 0.32\tp(negative) = 0.02\n",
      "----\n",
      "Sentence: The weather is not great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.74\tp(neutral) = 0.24\tp(negative) = 0.02\n",
      "----\n",
      "\n",
      "\n",
      "Positive Phrase Negations: Negative\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    5 (5.1%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I hate that the weather is good.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.55\tp(neutral) = 0.37\tp(negative) = 0.08\n",
      "----\n",
      "Sentence: I hate that the weather is nice.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.56\tp(neutral) = 0.39\tp(negative) = 0.05\n",
      "----\n",
      "Sentence: I hate that the weather is great.\n",
      "Gold: 0\t\tPredicted: 1\n",
      "p(positive) = 0.53\tp(neutral) = 0.33\tp(negative) = 0.14\n",
      "----\n",
      "\n",
      "\n",
      "Negative Adjective Negations: Positive\n",
      "Language: English\n",
      "Test cases:      33\n",
      "Fails (rate):    33 (100.0%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: The picture is not great.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.52\tp(neutral) = 0.32\tp(negative) = 0.16\n",
      "----\n",
      "Sentence: The game is not nice.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.72\tp(neutral) = 0.24\tp(negative) = 0.05\n",
      "----\n",
      "Sentence: The man is not nice.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.72\tp(neutral) = 0.25\tp(negative) = 0.02\n",
      "----\n",
      "\n",
      "\n",
      "Negative Phrases with Positive Additions: Positive\n",
      "Language: English\n",
      "Test cases:      99\n",
      "Fails (rate):    83 (83.8%)\n",
      "\n",
      "Example fails:\n",
      "Sentence: I'm sure that the world is boring.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.74\tp(neutral) = 0.16\tp(negative) = 0.11\n",
      "----\n",
      "Sentence: I love that the man is stupid.\n",
      "Gold: 2\t\tPredicted: 1\n",
      "p(positive) = 0.43\tp(neutral) = 0.35\tp(negative) = 0.22\n",
      "----\n",
      "Sentence: I'm sure that the world is stupid.\n",
      "Gold: 2\t\tPredicted: 0\n",
      "p(positive) = 0.61\tp(neutral) = 0.28\tp(negative) = 0.11\n",
      "----\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_example(x, pred, conf, label, meta=None): \n",
    "    return f\"Sentence: {x}\\nGold: {label}\\t\\tPredicted: {pred}\\np(positive) = {round(conf[0], 2)}\\tp(neutral) = {round(conf[1], 2)}\\tp(negative) = {round(conf[2], 2)}\"\n",
    "\n",
    "suite.summary(format_example_fn = format_example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trustenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
